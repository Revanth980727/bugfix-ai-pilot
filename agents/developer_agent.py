
import os
import time
import json
import re
import difflib
import tempfile
import subprocess
from typing import Dict, Any, List, Optional, Tuple, Union
import openai
from .utils.logger import Logger
from .utils.openai_client import OpenAIClient

class DeveloperAgent:
    """
    Agent responsible for generating code fixes based on PlannerAgent's task plan.
    Uses GPT-4 to generate code patches and handles multiple retry attempts.
    """
    
    def __init__(self, max_retries: int = 4):
        """
        Initialize the developer agent
        
        Args:
            max_retries: Maximum number of retry attempts for generating a successful patch
        """
        self.logger = Logger("developer_agent")
        self.max_retries = max_retries
        
        # Get repo path from environment
        self.repo_path = os.environ.get("REPO_PATH", "/mnt/codebase")
        
        # Initialize OpenAI client
        self.openai_client = OpenAIClient()
        
        # Get patch mode from environment (intelligent, line-by-line, direct)
        self.patch_mode = os.environ.get("PATCH_MODE", "line-by-line")
        self.logger.info(f"Using patch mode: {self.patch_mode}")
        
        # Check if required libraries are available
        self.has_unidiff = self._check_module_available("unidiff")
        self.has_diff_match_patch = self._check_module_available("diff_match_patch")
        
        if not self.has_unidiff:
            self.logger.warning("unidiff module not available, patch parsing will use fallback method")
        
    def _check_module_available(self, module_name: str) -> bool:
        """Check if a Python module is available"""
        try:
            __import__(module_name)
            return True
        except ImportError:
            return False
    
    def run(self, task_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate code fixes based on the task plan
        
        Args:
            task_plan: The task plan generated by PlannerAgent including context with previous attempts
                       and QA failure information
            
        Returns:
            Dictionary with patch information:
            {
                "patch_content": "unified diff content",
                "patched_files": ["list", "of", "patched", "files"],
                "patched_code": {"file_path": "content", ...},
                "commit_message": "Description of changes",
                "attempt": 1,
                "success": true
            }
        """
        # Ensure we have a valid task_plan
        if not task_plan:
            self.logger.error("Invalid task plan: None or empty")
            return {
                "error": "Invalid task plan provided",
                "patch_content": "",
                "patched_files": [],
                "patched_code": {},
                "commit_message": "",
                "attempt": 1,
                "ticket_id": "unknown",
                "success": False
            }
            
        ticket_id = task_plan.get("ticket_id", "unknown")
        self.logger.start_task(f"Generating code fix for ticket {ticket_id}")
        
        # Extract context if available
        context = task_plan.get("context", {}) or {}  # Ensure context is a dict, not None
        attempt = context.get("attempt", 1)
        previous_attempts = context.get("previousAttempts", []) or []  # Ensure previous_attempts is a list, not None
            
        self.logger.info(f"Starting fix attempt {attempt}/{self.max_retries}")
        
        try:
            # Generate the fix using OpenAI
            patch_data = self.generate_fix(task_plan, attempt, previous_attempts)
            
            # Add metadata
            patch_data["attempt"] = attempt
            patch_data["ticket_id"] = ticket_id
            patch_data["patch_mode"] = self.patch_mode
            
            # Validate that patched_files match the ones in patch_content
            patch_files_from_diff = self._extract_files_from_patch(patch_data.get("patch_content", ""))
            patch_files_declared = patch_data.get("patched_files", [])
            
            # Add any missing files from the diff to the patched_files list
            for file in patch_files_from_diff:
                if file not in patch_files_declared:
                    self.logger.warning(f"File {file} found in patch but not declared in patched_files, adding it")
                    patch_data["patched_files"].append(file)
            
            # Validate that all declared patched files have corresponding code
            patched_code = patch_data.get("patched_code", {})
            for file in patch_data["patched_files"]:
                if file not in patched_code:
                    self.logger.warning(f"File {file} declared in patched_files but no patched code provided")
            
            # Save patch to file
            patch_file_path = f"logs/patch_{ticket_id}_attempt_{attempt}.patch"
            os.makedirs("logs", exist_ok=True)
            with open(patch_file_path, "w") as f:
                f.write(patch_data.get("patch_content", "") or "")  # Handle None case
                
            self.logger.info(f"Patch saved to {patch_file_path}")
            
            # Set success status (actual verification happens in QA agent)
            patch_data["success"] = True
            
            # Add validation analytics if the patch contains at least one file
            validation_results = {}
            if patch_data.get("patch_content") and patch_data.get("patched_files") and patch_data.get("patched_code"):
                validation_results = self._validate_patch(
                    patch_data["patch_content"], 
                    patch_data["patched_files"],
                    patch_data["patched_code"]
                )
                patch_data["validation_results"] = validation_results
                
                if not validation_results.get("valid"):
                    self.logger.warning("Patch validation found issues, applying adaptations may be needed")
                    patch_data["warnings"] = [
                        "Patch validation detected potential issues that may require intelligent patching"
                    ]
                else:
                    self.logger.info("Patch validation successful")
            
            self.logger.end_task(f"Code generation for ticket {ticket_id}", success=True)
            return patch_data
            
        except Exception as e:
            self.logger.error(f"Code generation failed: {str(e)}")
            self.logger.end_task(f"Code generation for ticket {ticket_id}", success=False)
            
            # Return error response
            return {
                "error": str(e),
                "patch_content": "",
                "patched_files": [],
                "patched_code": {},
                "commit_message": "",
                "attempt": attempt,
                "ticket_id": ticket_id,
                "success": False
            }
    
    def generate_fix(self, task_plan: Dict[str, Any], attempt: int, previous_attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Generate a fix based on the task plan using the OpenAI API
        
        Args:
            task_plan: Dictionary with task plan from PlannerAgent
            attempt: Current attempt number
            previous_attempts: List of previous attempts
        
        Returns:
            Dictionary with patch information
        """
        # This method would use the OpenAI API to generate a fix
        # In a real implementation, this would:
        # 1. Craft a prompt based on the task plan
        # 2. Call OpenAI API to generate a fix
        # 3. Parse the response to extract the patch content
        # 4. Handle failures and retries
        
        # For now, we'll create a mock response
        self.logger.info("Generating code fix using OpenAI API (mocked)")
        
        # In a real system, we would use the OpenAI API here
        # For now, we'll create a simple patch based on the task plan
        ticket_id = task_plan.get("ticket_id", "unknown")
        summary = task_plan.get("summary", "")
        
        # Mock file changes
        file_changes = []
        for file in task_plan.get("affected_files", []):
            file_path = file if isinstance(file, str) else file.get("file", "")
            if not file_path:
                continue
                
            file_changes.append({
                "file_path": file_path,
                "diff": f"--- a/{file_path}\n+++ b/{file_path}\n@@ -1,3 +1,6 @@\n+# Fixed bug from ticket {ticket_id}: {summary}\n+\n def example_function():\n-    # Original code with bug\n-    return None\n+    # Fixed implementation\n+    return 'Bug fixed'"
            })
                
        # Mock test generation
        test_files = {}
        for file_change in file_changes:
            file_path = file_change["file_path"]
            if file_path.endswith(".py"):
                file_name = os.path.basename(file_path)
                file_name_without_ext = os.path.splitext(file_name)[0]
                test_file_name = f"test_{file_name_without_ext}.py"
                
                test_files[test_file_name] = f"""
import pytest

def test_example_function():
    from {file_name_without_ext} import example_function
    assert example_function() == 'Bug fixed'
"""
        
        # Extract the expected patched code for each file
        patched_code = {}
        for file_change in file_changes:
            file_path = file_change["file_path"]
            diff = file_change["diff"]
            # In a real system, we'd read the original file and apply the patch
            # For now, just extract the added lines as the expected content
            patched_content = self._mock_extract_patched_content(diff)
            patched_code[file_path] = patched_content
        
        # Apply the changes (but in production code, we'd let the patching system do this)
        results = self._apply_patch(file_changes)
        
        # Create response
        patch_data = {
            "patch_content": "\n".join(fc["diff"] for fc in file_changes),
            "patched_files": [fc["file_path"] for fc in file_changes],
            "patched_code": patched_code,
            "test_code": test_files,  # Add generated tests
            "commit_message": f"Fix {ticket_id}: {summary[:50]}",
            "confidence_score": 80,  # Mock confidence score
        }
        
        return patch_data
    
    def _mock_extract_patched_content(self, diff: str) -> str:
        """Extract patched content from a diff for mocking purposes"""
        # This is a very simplified implementation for the mock system
        # In real code, we'd actually apply the diff to the original content
        lines = []
        for line in diff.splitlines():
            if line.startswith('+') and not line.startswith('+++'):
                lines.append(line[1:])  # Remove the + prefix
            elif not line.startswith('-') and not line.startswith('---') and not line.startswith('@@'):
                lines.append(line)
                
        return "\n".join(lines)
        
    def _extract_files_from_patch(self, patch_content: str) -> List[str]:
        """Extract file paths from a patch"""
        files = []
        lines = patch_content.splitlines()
        
        for line in lines:
            if line.startswith("+++ b/") or line.startswith("+++ "):
                # Extract file path
                if line.startswith("+++ b/"):
                    file_path = line[6:]
                else:
                    file_path = line[4:]
                    
                if file_path and file_path not in files:
                    files.append(file_path)
                    
        return files
            
    def _validate_patch(
        self, 
        patch_content: str, 
        patched_files: List[str],
        patched_code: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        Validate that a patch matches the expected patched code
        
        Args:
            patch_content: The unified diff patch content
            patched_files: List of files that are expected to be patched
            patched_code: Dictionary mapping file paths to their expected content after patching
            
        Returns:
            Dictionary with validation results
        """
        results = {
            "valid": True,
            "errors": [],
            "files_validated": 0,
            "files_with_issues": 0,
            "file_results": {}
        }
        
        # First check if patch_content is empty
        if not patch_content or patch_content.isspace():
            results["valid"] = False
            results["errors"].append("Patch content is empty")
            return results
            
        # Check if all patched_files are in patch_content
        files_in_patch = self._extract_files_from_patch(patch_content)
        for file in patched_files:
            if file not in files_in_patch:
                results["valid"] = False
                results["errors"].append(f"File {file} is in patched_files but not in patch_content")
                results["files_with_issues"] += 1
                results["file_results"][file] = {
                    "valid": False,
                    "error": "File missing from patch content"
                }
            
        # For each file in the patch content, try applying the patch to empty content
        # and see if it matches the expected patched_code
        for file in files_in_patch:
            if file not in patched_code:
                results["valid"] = False
                results["errors"].append(f"File {file} is in patch_content but has no corresponding patched_code")
                results["files_with_issues"] += 1
                results["file_results"][file] = {
                    "valid": False,
                    "error": "No expected patched code provided"
                }
                continue
                
            # Extract file-specific patch
            file_patch = self._extract_file_patch(patch_content, file)
            if not file_patch:
                results["valid"] = False
                results["errors"].append(f"Failed to extract patch for {file}")
                results["files_with_issues"] += 1
                results["file_results"][file] = {
                    "valid": False,
                    "error": "Failed to extract patch"
                }
                continue
                
            # Try to read the original file
            try:
                file_path = os.path.join(self.repo_path, file.lstrip('/'))
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        original_content = f.read()
                else:
                    # For new files, use empty string as original content
                    original_content = ""
            except Exception as e:
                self.logger.warning(f"Error reading original file {file}: {str(e)}")
                original_content = ""
                
            # Apply patch with various strategies and check the result
            try:
                patch_success, patched_content, method = self._apply_patch_to_content(original_content, file_patch, file)
                
                # Compare patched_content with expected content
                expected_content = patched_code[file]
                
                # Normalize both contents to account for line ending differences
                expected_normalized = self._normalize_content(expected_content)
                patched_normalized = self._normalize_content(patched_content)
                
                if expected_normalized == patched_normalized:
                    results["files_validated"] += 1
                    results["file_results"][file] = {
                        "valid": True,
                        "method": method
                    }
                else:
                    results["valid"] = False
                    results["files_with_issues"] += 1
                    results["errors"].append(f"File {file}: Patched content doesn't match expected content")
                    results["file_results"][file] = {
                        "valid": False,
                        "error": "Content mismatch",
                        "method": method,
                        "patch_success": patch_success
                    }
            except Exception as e:
                results["valid"] = False
                results["files_with_issues"] += 1
                results["errors"].append(f"File {file}: Error validating patch: {str(e)}")
                results["file_results"][file] = {
                    "valid": False,
                    "error": f"Validation error: {str(e)}"
                }
                
        return results
    
    def _normalize_content(self, content: str) -> str:
        """Normalize content for comparison by removing whitespace and empty lines"""
        if not content:
            return ""
            
        # Convert all line endings to \n
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # Remove leading and trailing whitespace from each line
        lines = [line.strip() for line in content.splitlines()]
        
        # Remove empty lines
        lines = [line for line in lines if line]
        
        return '\n'.join(lines)
        
    def _extract_file_patch(self, patch_content: str, file_path: str) -> str:
        """Extract patch content for a specific file from a multi-file patch"""
        file_patch = ""
        in_target_file = False
        next_file_marker = re.compile(r'^--- a/[^\n]+$', re.MULTILINE)
        
        lines = patch_content.splitlines(True)  # keep line endings
        
        for i, line in enumerate(lines):
            # Look for the start of our target file's patch
            if line.startswith(f"--- a/{file_path}") or line.startswith(f"--- {file_path}"):
                in_target_file = True
                file_patch += line
                
                # Add the +++ line that should follow
                if i+1 < len(lines) and (lines[i+1].startswith(f"+++ b/{file_path}") or lines[i+1].startswith(f"+++ {file_path}")):
                    file_patch += lines[i+1]
                    
            # If we're in the target file's patch, keep adding lines until we hit the next file
            elif in_target_file:
                # Check if this is the start of the next file's patch
                if next_file_marker.match(line) and not line.startswith(f"--- a/{file_path}") and not line.startswith(f"--- {file_path}"):
                    break
                file_patch += line
                
        return file_patch
    
    def _apply_patch_to_content(self, original_content: str, patch_content: str, file_path: str) -> Tuple[bool, str, str]:
        """
        Apply a patch to file content using multiple strategies
        
        Args:
            original_content: Original file content as string
            patch_content: Unified diff patch content
            file_path: Path of the file being patched
            
        Returns:
            Tuple of (success, result_content, method_used)
        """
        self.logger.info(f"Applying patch to {file_path} using layered strategies")
        
        if not patch_content or patch_content.isspace():
            self.logger.warning(f"Empty patch for {file_path}, returning original content")
            return False, original_content, "no_patch_content"
        
        # Strategy 1: Try using unidiff if available
        if self.has_unidiff:
            try:
                import unidiff
                from io import StringIO
                
                # Parse the patch
                patch_set = unidiff.PatchSet(StringIO(patch_content))
                
                # For validation purposes, apply the patch in memory
                original_lines = original_content.splitlines()
                patched_lines = original_lines.copy()
                
                for patched_file in patch_set:
                    for hunk in patched_file:
                        hunk_start = hunk.target_start - 1  # 0-indexed
                        
                        # Apply the hunk changes
                        target_lines = []
                        for line in hunk:
                            if line.is_context:
                                target_lines.append(line.value)
                            elif line.is_added:
                                target_lines.append(line.value)
                                
                        # Replace the hunk in the patched lines
                        hunk_end = hunk_start + hunk.target_length
                        patched_lines[hunk_start:hunk_end] = target_lines
                
                patched_content = '\n'.join(patched_lines)
                self.logger.info(f"Successfully applied patch to {file_path} using unidiff")
                return True, patched_content, "unidiff"
                
            except Exception as e:
                self.logger.warning(f"Failed to apply patch using unidiff: {str(e)}")
                # Continue to next strategy
        
        # Strategy 2: Try using enhanced basic parser with context
        try:
            # Parse the patch into hunks
            hunks = self._parse_patch_hunks(patch_content)
            if not hunks:
                self.logger.warning(f"No valid hunks found in patch for {file_path}")
            else:
                # Apply each hunk to the content
                result_content = original_content
                for hunk in hunks:
                    result_content = self._apply_hunk_with_context(result_content, hunk)
                    
                # Check if content actually changed
                if result_content != original_content:
                    self.logger.info(f"Successfully applied patch to {file_path} using enhanced parser")
                    return True, result_content, "enhanced_parser"
                else:
                    self.logger.warning(f"Enhanced parser did not modify content for {file_path}")
        except Exception as e:
            self.logger.warning(f"Error during enhanced patch parsing: {str(e)}")
        
        # Strategy 3: Try using diff-match-patch if available
        if self.has_diff_match_patch:
            try:
                from diff_match_patch import diff_match_patch
                dmp = diff_match_patch()
                
                # Simplify patch to create patches usable by diff-match-patch
                # This is a very simplified approach - a real implementation should
                # properly convert unified diffs to diff-match-patch format
                patch_lines = patch_content.splitlines()
                
                # Extract just the content changes
                lines_to_remove = []
                lines_to_add = []
                for line in patch_lines:
                    if line.startswith('-') and not line.startswith('---'):
                        lines_to_remove.append(line[1:])
                    elif line.startswith('+') and not line.startswith('+++'):
                        lines_to_add.append(line[1:])
                
                # Try to apply fuzzy patching
                # This is a very basic implementation - a real one would be more sophisticated
                if lines_to_remove or lines_to_add:
                    # Create a simplistic patch text
                    patch_text = ""
                    if lines_to_remove:
                        patch_text += "".join(lines_to_remove)
                    if lines_to_add:
                        patch_text = "".join(lines_to_add)
                        
                    # Use diff-match-patch to create patches
                    patches = dmp.patch_make(original_content, patch_text)
                    patched_content, results = dmp.patch_apply(patches, original_content)
                    
                    # Check if any patches were applied
                    if patched_content != original_content:
                        self.logger.info(f"Successfully applied patches to {file_path} using diff-match-patch")
                        return True, patched_content, "diff_match_patch"
                    else:
                        self.logger.warning(f"diff-match-patch did not modify content for {file_path}")
            except Exception as e:
                self.logger.warning(f"Error during diff-match-patch: {str(e)}")
        
        # Strategy 4: Try line by line patching
        try:
            result_content = self._apply_line_by_line_changes(original_content, patch_content)
            if result_content != original_content:
                self.logger.info(f"Successfully applied patch to {file_path} using line-by-line strategy")
                return True, result_content, "line_by_line"
            else:
                self.logger.warning(f"Line-by-line patching did not modify content for {file_path}")
        except Exception as e:
            self.logger.warning(f"Error during line-by-line patching: {str(e)}")
        
        # Fallback strategy: Extract content from patch (for new files)
        try:
            clean_content = self._clean_diff_markers(patch_content)
            if clean_content and not clean_content.isspace() and clean_content != original_content:
                self.logger.info(f"Applied patch to {file_path} using clean content extraction")
                return True, clean_content, "clean_extraction"
            else:
                self.logger.warning(f"Clean content extraction yielded empty or unchanged result for {file_path}")
        except Exception as e:
            self.logger.warning(f"Error during clean content extraction: {str(e)}")
        
        # If all strategies failed
        self.logger.error(f"All patch strategies failed for {file_path}")
        return False, original_content, "all_failed"
        
    def _parse_patch_hunks(self, patch_content: str) -> List[Dict[str, Any]]:
        """
        Parse a unified diff patch into structured hunks
        """
        hunks = []
        current_hunk = None
        
        # Regular expression to match hunk header
        hunk_header_pattern = re.compile(r'^@@ -(\d+)(?:,(\d+))? \+(\d+)(?:,(\d+))? @@(.*)$')
        
        for line in patch_content.splitlines():
            # Skip file headers
            if line.startswith('---') or line.startswith('+++'):
                continue
                
            # Parse hunk header
            match = hunk_header_pattern.match(line)
            if match:
                # Start a new hunk
                start_line = int(match.group(1))
                line_count = int(match.group(2) or 1)
                new_start = int(match.group(3))
                new_count = int(match.group(4) or 1)
                hunk_desc = match.group(5).strip() if match.group(5) else ""
                
                current_hunk = {
                    'start_line': start_line,
                    'line_count': line_count,
                    'new_start': new_start,
                    'new_count': new_count,
                    'description': hunk_desc,
                    'context_before': [],
                    'context_after': [],
                    'lines_removed': [],
                    'lines_added': []
                }
                hunks.append(current_hunk)
                
            elif current_hunk is not None:
                # Add lines to the current hunk
                if line.startswith(' '):
                    # Context line
                    if not current_hunk['lines_removed'] and not current_hunk['lines_added']:
                        # Context before changes
                        current_hunk['context_before'].append(line[1:])
                    else:
                        # Context after changes
                        current_hunk['context_after'].append(line[1:])
                        
                elif line.startswith('-'):
                    # Line removed
                    current_hunk['lines_removed'].append(line[1:])
                    
                elif line.startswith('+'):
                    # Line added
                    current_hunk['lines_added'].append(line[1:])
        
        return hunks
        
    def _apply_hunk_with_context(self, content: str, hunk: Dict[str, Any]) -> str:
        """
        Apply a parsed hunk to content with context matching
        """
        lines = content.splitlines()
        result_lines = []
        
        # Try to find the correct location for the hunk
        context_before = hunk['context_before']
        context_after = hunk['context_after']
        lines_removed = hunk['lines_removed']
        lines_added = hunk['lines_added']
        target_line = hunk['start_line'] - 1  # 0-indexed
        
        # If we have context before, try to find exact match for better positioning
        best_position = None
        max_context_matched = -1
        
        # Try to find the best position based on context matching
        for i in range(len(lines)):
            # Skip if we don't have enough lines left
            if i + len(context_before) + len(lines_removed) > len(lines):
                continue
                
            # Check if context before matches at this position
            context_matches = sum(1 for j, ctx_line in enumerate(context_before) 
                               if i + j < len(lines) and lines[i + j] == ctx_line)
                               
            # Check if removed lines match after context
            removed_matches = sum(1 for j, rm_line in enumerate(lines_removed) 
                               if i + len(context_before) + j < len(lines) 
                               and lines[i + len(context_before) + j] == rm_line)
            
            # Calculate total match score
            match_score = (context_matches / max(1, len(context_before))) * 0.7 + \
                          (removed_matches / max(1, len(lines_removed))) * 0.3
                          
            # If this is the best match so far and it's reasonable
            if match_score > max_context_matched and match_score > 0.5:
                max_context_matched = match_score
                best_position = i
                
        # If we couldn't find a good match, try just near the target line
        if best_position is None:
            target_area_start = max(0, min(target_line, len(lines) - 1))
            search_radius = 10  # Look 10 lines before and after target
            
            for i in range(max(0, target_area_start - search_radius), 
                           min(len(lines), target_area_start + search_radius)):
                # Check if any context line matches at this position
                for ctx_line in context_before:
                    if i < len(lines) and lines[i] == ctx_line:
                        best_position = i
                        break
                if best_position is not None:
                    break
                    
        # If still no match, use the target line from the patch
        if best_position is None:
            best_position = min(target_line, len(lines))
            
        # Apply the hunk
        # Copy lines up to the hunk position
        result_lines.extend(lines[:best_position])
        
        # Skip context before (already verified it matches)
        skip_lines = len(context_before) + len(lines_removed)
        
        # Add the new lines
        result_lines.extend(lines_added)
        
        # Continue with the rest of the file
        if best_position + skip_lines < len(lines):
            result_lines.extend(lines[best_position + skip_lines:])
            
        return '\n'.join(result_lines)

    def _apply_patch(self, file_changes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Apply patches to the repository files with precise line-by-line changes"""
        results = {
            "files_modified": [],
            "files_failed": [],
            "patches_applied": 0,
            "patched_code": {},  # Will store the actual patched code content
            "patch_mode": self.patch_mode
        }
        
        for change in file_changes:
            file_path = change.get("file_path", "unknown")
            diff = change.get("diff", "")
            
            # Skip if the file path is unknown or diff is empty
            if file_path == "unknown":
                self.logger.error(f"Skipping unknown file path")
                results["files_failed"].append({
                    "file": "unknown",
                    "reason": "File path could not be determined"
                })
                continue
                
            if not diff:
                self.logger.error(f"Skipping empty diff for file {file_path}")
                results["files_failed"].append({
                    "file": file_path,
                    "reason": "Empty diff content"
                })
                continue
            
            # Construct the full path
            full_path = os.path.join(self.repo_path, file_path.lstrip('/'))
            
            try:
                # Check if file exists
                if not os.path.exists(full_path):
                    # If it's a new file
                    dir_path = os.path.dirname(full_path)
                    if not os.path.exists(dir_path):
                        os.makedirs(dir_path)
                        
                    self.logger.info(f"Creating new file: {file_path}")
                    with open(full_path, 'w') as f:
                        # Remove diff markers before writing to a new file
                        clean_content = self._clean_diff_markers(diff)
                        f.write(clean_content)
                    
                    results["files_modified"].append(file_path)
                    results["patches_applied"] += 1
                    results["patched_code"][file_path] = clean_content
                    continue
                
                # For existing files, read the content
                with open(full_path, 'r', encoding='utf-8') as f:
                    original_content = f.read()
                
                # Apply patch based on selected mode
                success, patched_content, method = self._apply_patch_to_content(original_content, diff, file_path)
                
                if success:
                    # Skip if no changes were made
                    if patched_content == original_content:
                        self.logger.warning(f"No changes were applied to {file_path} using {method}")
                        results["files_failed"].append({
                            "file": file_path,
                            "reason": "No changes could be applied"
                        })
                        continue
                        
                    # Write the modified content back
                    with open(full_path, 'w', encoding='utf-8') as f:
                        f.write(patched_content)
                    
                    # Store original and patched code
                    results["patched_code"][file_path] = patched_content
                    
                    # Log success
                    self.logger.info(f"Successfully patched file: {file_path} using {method}")
                    results["files_modified"].append(file_path)
                    results["patches_applied"] += 1
                else:
                    self.logger.error(f"Failed to patch {file_path} using any method")
                    results["files_failed"].append({
                        "file": file_path,
                        "reason": f"All patch methods failed"
                    })
                
            except Exception as e:
                error_msg = f"Failed to patch {file_path}: {str(e)}"
                self.logger.error(error_msg)
                results["files_failed"].append({
                    "file": file_path,
                    "reason": str(e)
                })
        
        return results
        
    def _clean_diff_markers(self, content: str) -> str:
        """Remove diff markers and metadata from content when it's a complete file"""
        # If this doesn't look like a diff with markers, return as is
        if not any(line.startswith(('+', '-', '@@ ')) for line in content.splitlines()):
            return content
        
        # If it looks like a unified diff, extract only added lines (without the + marker)
        clean_lines = []
        for line in content.splitlines():
            # Skip diff metadata lines and removal lines
            if line.startswith('@@') or line.startswith('-') or line.startswith('---') or line.startswith('+++'):
                continue
            # Include added lines (without the + marker)
            elif line.startswith('+'):
                clean_lines.append(line[1:])
            # Include context lines (without markers)
            elif not line.startswith(('-', '+')):
                clean_lines.append(line)
        
        return '\n'.join(clean_lines)

    def _apply_line_by_line_changes(self, original_content: str, diff: str) -> str:
        """
        Apply diff changes line by line to preserve most of the original file
        
        Args:
            original_content: The original file content
            diff: The diff content with - and + prefixes for line changes
            
        Returns:
            Modified content with changes applied
        """
        self.logger.info("Applying changes using line-by-line patching strategy")
        
        # Check if this is a proper diff format with +/- markers
        has_diff_markers = any(line.startswith('+') or line.startswith('-') 
                              for line in diff.splitlines())
        
        if not has_diff_markers:
            self.logger.warning("No diff markers found, returning original content")
            return original_content
            
        # Parse the diff to identify line changes
        original_lines = original_content.splitlines()
        result_lines = original_lines.copy()
        
        # Process diff lines to extract changes
        diff_lines = diff.splitlines()
        line_idx = 0  # Pointer to the current line in original content
        
        # Track specific added and removed lines precisely
        lines_to_remove = []  # List of (line_number, content) tuples
        lines_to_add = []     # List of (insert_position, content) tuples
        
        # First, identify the lines we need to remove
        for i, line in enumerate(diff_lines):
            if line.startswith('-') and not line.startswith('---'):
                line_content = line[1:]  # Remove the - marker
                # Try to find this line in the original content
                found = False
                
                # First, check if the next line in our current position matches
                if line_idx < len(original_lines) and line_content.strip() == original_lines[line_idx].strip():
                    lines_to_remove.append((line_idx, line_content))
                    found = True
                    
                # If not found at current position, search nearby lines
                if not found:
                    # Search in a window around our current position
                    window_size = 10
                    search_start = max(0, line_idx - window_size)
                    search_end = min(len(original_lines), line_idx + window_size)
                    
                    for j in range(search_start, search_end):
                        if original_lines[j].strip() == line_content.strip():
                            lines_to_remove.append((j, line_content))
                            found = True
                            # Update line_idx to continue from the line after the removed one
                            line_idx = j + 1
                            break
                            
                if not found:
                    self.logger.warning(f"Could not find line to remove: '{line_content[:40]}...'")
        
        # Now identify the lines we need to add
        for i, line in enumerate(diff_lines):
            if line.startswith('+') and not line.startswith('+++'):
                line_content = line[1:]  # Remove the + marker
                
                # Try to find the best position for this addition
                insert_position = line_idx
                
                # If this addition follows a removal, insert at the position of the removal
                if i > 0 and diff_lines[i-1].startswith('-') and not diff_lines[i-1].startswith('---'):
                    removal_content = diff_lines[i-1][1:]
                    
                    # Find the corresponding removal in our tracked removals
                    for pos, content in lines_to_remove:
                        if content.strip() == removal_content.strip():
                            insert_position = pos
                            break
                
                lines_to_add.append((insert_position, line_content))
                
        # Sort lines to remove in reverse order to avoid index shifting
        lines_to_remove.sort(reverse=True)
        
        # Apply removals
        for line_num, _ in lines_to_remove:
            if 0 <= line_num < len(result_lines):
                del result_lines[line_num]
                
        # Sort additions by position
        lines_to_add.sort()
        
        # Apply additions with appropriate index adjustments
        offset = 0
        for orig_pos, content in lines_to_add:
            adjusted_pos = max(0, min(orig_pos + offset, len(result_lines)))
            result_lines.insert(adjusted_pos, content)
            offset += 1
        
        # If the diff didn't change anything, log a warning
        if result_lines == original_lines:
            self.logger.warning("Line-by-line patching did not change the file content")
            
            # Try a fallback approach - extract just the clean content from the diff
            clean_content = self._clean_diff_markers(diff)
            if clean_content and clean_content.strip() and clean_content != original_content:
                self.logger.info("Using fallback clean content extraction")
                return clean_content
        
        return '\n'.join(result_lines)
